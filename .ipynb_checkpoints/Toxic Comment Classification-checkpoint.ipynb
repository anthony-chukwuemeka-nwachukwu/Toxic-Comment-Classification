{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Outline\n",
    "---\n",
    "0. Import the necessary libraries\n",
    "1. Download or otherwise retrieve the data.\n",
    "2. Process / Prepare the data.\n",
    "3. Upload the processed data to S3.\n",
    "4. Train a chosen model.\n",
    "5. Test the trained model (typically using a batch transform job).\n",
    "6. Deploy the trained model.\n",
    "7. Use the deployed model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Importing Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install googletrans\n",
    "!pip install tqdm\n",
    "!pip install torch\n",
    "!pip install emoji\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shutil import unpack_archive\n",
    "import torch\n",
    "import helper as hp\n",
    "import os\n",
    "import ast\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to use GPU if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two challenges you might face while trying to download this data are:\n",
    "- Pulling the data from Kaggle directly to the notebook instance you might be working with\n",
    "- Memory issue if you are using Amazon SagemMaker\n",
    "I have tried to address these issues below.\n",
    "\n",
    "#### Install Kaggle library and downlaod Zip file\n",
    "To successfully pull the data directly to your notebook incase you are working on a remote device, you can take these steps:\n",
    "1. **pip install the Kaggle library**. You can do this by opening the notebook's terminal and running, `pip install the Kaggle library`\n",
    "2. **Create a directory** to put the API token in, so the Kaggle library will know where to look for your sign-in credentials when you try to access the API from your notebook. This directory must be named `.kaggle` and it must be in the same directory as your installation of Python, which is typically the home directory. Note that this file will be hidden. Do this on the terminal. `mkdir .kaggle`\n",
    "3. **Go to your Kaggle account** (or create one if you haven’t yet), click the profile icon in the top right corner of the screen, and select “my profile” from the dropdown list. Scroll down to about middle of the page, and click on “Create a New API Token”. Then, a file named kaggle.json will automatically download to your downloads folder (or your default download folder). This file contains your sign-in credentials to allow you to access the API.\n",
    "4. **Navigate to the directory** where the above file was downlaoded, you need to move the kaggle.json file to the new .kaggle directory. Do this on the terminal. `mv kaggle.json path_to_the_.kaggle_folder/.kaggle`\n",
    "5. **Now you can view list of kaggle competions** from your terminal. `kaggle competitions list`\n",
    "6. **You can down the competition zip folder** through your terminal directly to the location you are working at. `kaggle competitions download -c jigsaw-multilingual-toxic-comment-classification`. If you want to do the download from the notebook, `!kaggle competitions download -c jigsaw-multilingual-toxic-comment-classification`\n",
    "\n",
    "> I also recommend you go through this github link `https://github.com/floydwch/kaggle-cli`\n",
    "\n",
    "### Unzip the file\n",
    "One major challenged I faced while trying to unzip the file on Amazon SageMaker was memory issue. This was how I resolved that.\n",
    "1. **I went to Amazon SageMaker**, then to the Notebook Instance I was working on. I clicked on `edit`, then scrolled down to the `volume`. I increased the default volume from 5gb to like 160gb. Then updated. That solved the issue for me.\n",
    "2. **Then I proceeded** to the next cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpack_archive('../data/jigsaw-multilingual-toxic-comment-classification.zip', '../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate the data\n",
    "Since the `validation and test sets` are in many languages, we need to translate them to English for consistency using google translate API.\n",
    "The code for this can be found inside the helper function  \n",
    "> **Note** that translating could be a bit difficult. In my case, I used google API. The keep blocking it, I deviced a walkaround whereby when you send in the file for translation, it checks the column with the tittle `outcome`. If the value for that row is `success`, it does nothing to it, else (if it is `failure`, which is the default value), it attempts to translate it. If the API has block, it returns that row values as they are, else, it returns the row values with the content (or context) translated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vaildation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation set\n",
    "CSV_PATH = '../data/validation.csv'\n",
    "df = pd.read_csv(CSV_PATH)#, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column named outcome and populate it's rows with failure.\n",
    "# Save it as validation_ammended.csv\n",
    "df['outcome'] = df.apply(lambda row : 'failure', axis = 1)\n",
    "df.to_csv('../data/validation_ammended.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation_ammended.csv to see it's contents\n",
    "CSV_PATH  = '../data/validation_ammended.csv'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick here is to make the call with CSV_PATH as input for the first time. Then after the translation is done, you check if there are still **outcome** columns with `failure`, if there is, make the next call on `OUT_PUT_PATH`. Keep making the subsequent calls on `OUT_PUT_PATH` untill there is no more **outcome** column with `failure`.  \n",
    "> You do this check using `df.groupby('outcome').count()` as can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the translateDf function from preprocess.py file\n",
    "OUT_PUT_PATH = '../data/jigsaw-toxic-comment-validation-translated.csv'\n",
    "# First call with CSV_PATH\n",
    "#pr.translateDf(CSV_PATH, OUT_PUT_PATH, ['id','comment_text','lang','toxic','outcome']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsequent calls with OUT_PUT_PATH\n",
    "#pr.translateDf(OUT_PUT_PATH, OUT_PUT_PATH, ['id','comment_text','lang','toxic','outcome']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each call above, run the two cells below to check if there is still any **outcome** column with `failure` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and print the output file\n",
    "df = pd.read_csv(OUT_PUT_PATH)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the outcome column still have failure\n",
    "df.groupby('outcome').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, you can process in the two files below to **remove** the added `outcome` column, **sort** the data in ascending order based on the `id` and **save** the final result as `validation_translated_ordered.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the outcome column\n",
    "df.drop('outcome', axis=1, inplace=True)\n",
    "# Sort by id\n",
    "df.sort_values('id', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final validation translated file\n",
    "df.to_csv('../data/validation_translated_ordered.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doctor Who adlı viki başlığına 12. doctor olarak bir viki yazarı kendi adını eklemiştir. Şahsen düzelttim. Onaylarsanız sevinirim. Occipital '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test set\n",
    "CSV_PATH = '../data/test.csv'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column named outcome and populate it's rows with failure.\n",
    "# Save it as test_ammended.csv\n",
    "df['outcome'] = df.apply(lambda row : 'failure', axis = 1)\n",
    "df.to_csv('../data/test_ammended.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test_ammended.csv to see it's contents\n",
    "CSV_PATH  = '../data/test_ammended.csv'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick here is to make the call with CSV_PATH as input for the first time. Then after the translation is done, you check if there are still **outcome** columns with `failure`, if there is, make the next call on `OUT_PUT_PATH`. Keep making the subsequent calls on `OUT_PUT_PATH` untill there is no more **outcome** column with `failure`.  \n",
    "> You do this check using `df.groupby('outcome').count()` as can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the translateDf function from preprocess.py file\n",
    "OUT_PUT_PATH = '../data/jigsaw-toxic-comment-test-translated.csv'\n",
    "# First call with CSV_PATH\n",
    "#pr.translateDf(CSV_PATH, OUT_PUT_PATH, ['id','comment_text','lang','toxic','outcome']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsequent calls with OUT_PUT_PATH\n",
    "#pr.translateDf(OUT_PUT_PATH, OUT_PUT_PATH, ['id','comment_text','lang','toxic','outcome']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each call above, run the two cells below to check if there is still any **outcome** column with `failure` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and print the output file\n",
    "df = pd.read_csv(OUT_PUT_PATH)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the outcome column still have failure\n",
    "df.groupby('outcome').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, you can process in the two files below to **remove** the added `outcome` column, **sort** the data in ascending order based on the `id` and **save** the final result as `test_translated_ordered.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the outcome column\n",
    "df.drop('outcome', axis=1, inplace=True)\n",
    "# Sort by id\n",
    "df.sort_values('id', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final validation translated file\n",
    "df.to_csv('../data/test_translated_ordered.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract neccessary columns and Shuffle\n",
    "\n",
    "The code to extract neccessary columns and shuffle is included in the helper file. We will just make a call to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = '../data/jigsaw-toxic-comment-train.csv'\n",
    "data_valid = '../data/validation_translated_ordered.csv'\n",
    "data_test  = '../data/test_translated_ordered.csv'\n",
    "\n",
    "train_X,train_y, valid_X,valid_y,valid_lan, test_X,test_lan,test_id = hp.prepare_imdb_data(data_train, \\\n",
    "                                                                                           data_valid, data_test)\n",
    "\n",
    "print(\"Toxic comments (combined): train = {}, validation = {}, test = {}\".format(len(train_X), len(valid_X),\\\n",
    "                                                                                 len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `training`, `validation` and `testing` sets prepared, we should do a quick check and see an example of the data our model will be trained on. This is generally a good idea as it allows you to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X[100],'\\n-----------------------------\\n')\n",
    "print(valid_X[100],'\\n-----------------------------\\n')\n",
    "print(test_X[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove HTML tags and Tokenize\n",
    "\n",
    "Now, we want to make sure that any html tags that appear should be removed. In addition we wish to tokenize our input, that way words such as *entertained* and *entertaining* are considered the same with regard to sentiment analysis.\n",
    "This is the breakdown of what the function below does:\n",
    "1. It converts all words to lowercase\n",
    "2. Removes stopwords\n",
    "3. Removes punctuations\n",
    "4. Splits the string into list of words\n",
    "\n",
    "+ There is a method before the `preprocess` method named `review_to_words` which `preprocess` method applies to each of the user comments in the training, validation and testing datasets.\n",
    "+ In addition it caches the results. This is because performing this processing step can take a long time. This way if you are unable to complete the notebook in the current session, you can come back without needing to process the data a second time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We now get the list of all the words in training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = '../data/jigsaw-toxic-comment-train.csv'\n",
    "data_valid = '../data/validation_translated_ordered.csv'\n",
    "data_test  = '../data/test_translated_ordered.csv'\n",
    "OUT_PUT_PATH_train = '../data/words_train.csv'\n",
    "OUT_PUT_PATH_valid = '../data/words_validation.csv'\n",
    "OUT_PUT_PATH_test = '../data/words_test.csv'\n",
    "column_train = ['id','comment_text','toxic']\n",
    "column_valid = ['id','comment_text','lang','toxic']\n",
    "column_test = ['id','content','lang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hp.word_list_Df(OUT_PUT_PATH_train+'_1', OUT_PUT_PATH_train+'_1', column_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.word_list_Df(data_train, OUT_PUT_PATH_train, column_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(OUT_PUT_PATH_train)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_text'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('toxic').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build word_dictionary\n",
    "\n",
    "> - A careful examination of the contents of the dataset shows the lists created are wrapped around a string. We first extract the lists from the strings and build the `train_X, train_y, valid_X, valid_y, test_X and test_id`.\n",
    "> - Next we buit the dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(OUT_PUT_PATH_train)\n",
    "train_X = [0]*train.shape[0]\n",
    "j = 0\n",
    "for i,elem in enumerate(train['comment_text']):\n",
    "    try:\n",
    "        train_X[i] = ast.literal_eval(elem)\n",
    "    except:\n",
    "        j += 1\n",
    "        train_X[i] = 'None'\n",
    "train_y = list(train['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv(OUT_PUT_PATH_valid)\n",
    "valid_X = [0]*valid.shape[0]\n",
    "k = 0\n",
    "for i,elem in enumerate(valid['comment_text']):\n",
    "    try:\n",
    "        valid_X[i] = ast.literal_eval(elem)\n",
    "    except:\n",
    "        k += 1\n",
    "        valid_X[i] = 'None'\n",
    "valid_y = list(valid['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(OUT_PUT_PATH_test)\n",
    "test_X = [0]*test.shape[0]\n",
    "m = 0\n",
    "for i,elem in enumerate(test['content']):\n",
    "    try:\n",
    "        test_X[i] = ast.literal_eval(elem)\n",
    "    except:\n",
    "        m += 1\n",
    "        test_X[i] = 'None'\n",
    "test_id = list(test['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the dictionary of words with the Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of words\n",
    "word_dict = hp.build_dict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4998"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default vocab_size = 5000\n",
    "len(word_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['promo', 'cabl', 'lip', 'spade', 'vegetarian']\n"
     ]
    }
   ],
   "source": [
    "# Checking the most frequent words\n",
    "print(list({k: v for k, v in sorted(word_dict.items(), key=lambda item: item[1], reverse=True)}.keys())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save `word_dict`\n",
    "\n",
    "Later on when we construct an endpoint which processes a submitted review we will need to make use of the `word_dict` which we have created. As such, we will save it to a file now for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/pytorch' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the reviews\n",
    "\n",
    "Now that we have our word dictionary which allows us to transform the words appearing in the reviews into integers, it is time to make use of it and convert our reviews to their integer sequence representation, making sure to pad or truncate to a fixed length, which in our case is `500`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_X_len = hp.convert_and_pad_data(word_dict, train_X)\n",
    "valid_X, valid_X_len = hp.convert_and_pad_data(word_dict, valid_X)\n",
    "test_X, test_X_len = hp.convert_and_pad_data(word_dict, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick check to make sure that things are working as intended, check to see what one of the reviews in the training set looks like after having been processeed. Does this look reasonable? What is the length of a review in the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data is (223549, 500), while shape of train label is (223549,) \n",
      "\n",
      "One training data is 12 == 12\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\n",
    "print(\"Shape of train data is {}, while shape of train label is {} \\n\\nOne training data is {} == {}\" \\\n",
    "      .format(np.shape(train_X),np.shape(train_X_len), train_X_len[230], np.count_nonzero(train_X[230])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload the data to S3\n",
    "\n",
    "---\n",
    "\n",
    "We will need to upload the training dataset to S3 in order for our training code to access it. For now we will save it locally and we will upload to S3 later on.\n",
    "\n",
    "### Save the processed training dataset locally\n",
    "\n",
    "It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form `label`, `length`, `review[500]` where `review[500]` is a sequence of `500` integers representing the words in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(valid_y), pd.DataFrame(valid_X_len), pd.DataFrame(valid_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'valid.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(test_id), pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the training data\n",
    "\n",
    "\n",
    "Next, we need to upload the training data to the SageMaker default S3 bucket so that we can provide access to it while training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sentiment_rnn'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The cell above uploads the entire contents of our data directory. This includes the `word_dict.pkl` file. This is fortunate as we will need this later on when we create an endpoint that accepts an arbitrary review. For now, we will just take note of the fact that it resides in the data directory (and so also in the S3 training bucket) and that we will need to make sure it gets saved in the model directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build and Train the PyTorch Model\n",
    "\n",
    "In the XGBoost notebook we discussed what a model is in the SageMaker framework. In particular, a model comprises three objects\n",
    "\n",
    " - Model Artifacts,\n",
    " - Training Code, and\n",
    " - Inference Code,\n",
    " \n",
    "each of which interact with one another. In the XGBoost example we used training and inference code that was provided by Amazon. Here we will still be using containers provided by Amazon with the added benefit of being able to include our own custom code.\n",
    "\n",
    "I have implemented the neural network in PyTorch along with a training script. The model object is in the `model.py` file, inside of the `train` folder. You can see the implementation by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
