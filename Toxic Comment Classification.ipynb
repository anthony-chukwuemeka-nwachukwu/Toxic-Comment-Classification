{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Outline\n",
    "---\n",
    "0. Import the necessary libraries\n",
    "1. Download or otherwise retrieve the data.\n",
    "2. Process / Prepare the data.\n",
    "3. Upload the processed data to S3.\n",
    "4. Train a chosen model.\n",
    "5. Test the trained model (typically using a batch transform job).\n",
    "6. Deploy the trained model.\n",
    "7. Use the deployed model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Importing Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install googletrans\n",
    "#!pip install tqdm\n",
    "#!pip install torch\n",
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shutil import unpack_archive\n",
    "import torch\n",
    "import helper as hp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to use GPU if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two challenges you might face while trying to download this data are:\n",
    "- Pulling the data from Kaggle directly to the notebook instance you might be working with\n",
    "- Memory issue if you are using Amazon SagemMaker\n",
    "I have tried to address these issues below.\n",
    "\n",
    "#### Install Kaggle library and downlaod Zip file\n",
    "To successfully pull the data directly to your notebook incase you are working on a remote device, you can take these steps:\n",
    "1. **pip install the Kaggle library**. You can do this by opening the notebook's terminal and running, `pip install the Kaggle library`\n",
    "2. **Create a directory** to put the API token in, so the Kaggle library will know where to look for your sign-in credentials when you try to access the API from your notebook. This directory must be named `.kaggle` and it must be in the same directory as your installation of Python, which is typically the home directory. Note that this file will be hidden. Do this on the terminal. `mkdir .kaggle`\n",
    "3. **Go to your Kaggle account** (or create one if you haven’t yet), click the profile icon in the top right corner of the screen, and select “my profile” from the dropdown list. Scroll down to about middle of the page, and click on “Create a New API Token”. Then, a file named kaggle.json will automatically download to your downloads folder (or your default download folder). This file contains your sign-in credentials to allow you to access the API.\n",
    "4. **Navigate to the directory** where the above file was downlaoded, you need to move the kaggle.json file to the new .kaggle directory. Do this on the terminal. `mv kaggle.json path_to_the_.kaggle_folder/.kaggle`\n",
    "5. **Now you can view list of kaggle competions** from your terminal. `kaggle competitions list`\n",
    "6. **You can down the competition zip folder** through your terminal directly to the location you are working at. `kaggle competitions download -c jigsaw-multilingual-toxic-comment-classification`. If you want to do the download from the notebook, `!kaggle competitions download -c jigsaw-multilingual-toxic-comment-classification`\n",
    "\n",
    "> I also recommend you go through this github link `https://github.com/floydwch/kaggle-cli`\n",
    "\n",
    "### Unzip the file\n",
    "One major challenged I faced while trying to unzip the file on Amazon SageMaker was memory issue. This was how I resolved that.\n",
    "1. **I went to Amazon SageMaker**, then to the Notebook Instance I was working on. I clicked on `edit`, then scrolled down to the `volume`. I increased the default volume from 5gb to like 160gb. Then updated. That solved the issue for me.\n",
    "2. **Then I proceeded** to the next cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpack_archive('../data/jigsaw-multilingual-toxic-comment-classification.zip', '../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate the data\n",
    "Since the `validation and test sets` are in many languages, we need to translate them to English for consistency using google translate API.\n",
    "The code for this can be found inside the helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input and output paths\n",
    "CSV_PATH = '../data/validation.csv'\n",
    "out = '../data/jigsaw-toxic-comment-validation-translated.csv'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a call to the helper function\n",
    "\"\"\"Uncomment below to translate the data\"\"\"\n",
    "\n",
    "#out = '../data/jigsaw-toxic-comment-validation-translated.csv'\n",
    "#hp.translateDf(CSV_PATH,out,['id','comment_text','lang', 'toxic']);\n",
    "#out = '../data/jigsaw-toxic-comment-test-translated.csv'\n",
    "#hp.translateDf(CSV_PATH,out,['id','content','lang']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output of the translation\n",
    "out = '../data/test.csv'\n",
    "df = pd.read_csv(out)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract neccessary columns and Shuffle\n",
    "\n",
    "The code to extract neccessary columns and shuffle is included in the helper file. We will just make a call to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic comments (combined): train = 223549, validation = 8000, test = 63812\n"
     ]
    }
   ],
   "source": [
    "data_train = '../data/jigsaw-toxic-comment-train.csv'\n",
    "data_valid = '../data/jigsaw-toxic-comment-validation-translated.csv'\n",
    "data_test  = '../data/jigsaw-toxic-comment-test-translated.csv'\n",
    "\n",
    "train_X,train_y, valid_X,valid_y,valid_lan, test_X,test_lan,test_id = hp.prepare_imdb_data(data_train, data_valid, data_test)\n",
    "\n",
    "print(\"Toxic comments (combined): train = {}, validation = {}, test = {}\".format(len(train_X), len(valid_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `training`, `validation` and `testing` sets prepared, we should do a quick check and see an example of the data our model will be trained on. This is generally a good idea as it allows you to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, the Moonlite edit noted by golden daph was me (on optus ...)  Wake up wikkis.  So funny \n",
      "-----------------------------\n",
      "concerning spam was that rewrote thinking that the first time he had not done well, it must conclude that I must change the focus of the article ?, is not promotional but descriptive \n",
      "-----------------------------\n",
      "@ Lucretia Dashnak propaganda: are you talking about here? ENOUGH, fed up with your tone and your délires.Cesar Borgia \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print(train_X[100],'\\n-----------------------------')\n",
    "print(valid_X[100],'\\n-----------------------------')\n",
    "print(test_X[100],'\\n-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove HTML tags and Tokenize\n",
    "\n",
    "Now, we want to make sure that any html tags that appear should be removed. In addition we wish to tokenize our input, that way words such as *entertained* and *entertaining* are considered the same with regard to sentiment analysis.\n",
    "This is the breakdown of what the function below does:\n",
    "1. It converts all words to lowercase\n",
    "2. Removes stopwords\n",
    "3. Removes punctuations\n",
    "4. Splits the string into list of words\n",
    "\n",
    "+ There is a method before the `preprocess` method named `review_to_words` which `preprocess` method applies to each of the user comments in the training, validation and testing datasets.\n",
    "+ In addition it caches the results. This is because performing this processing step can take a long time. This way if you are unable to complete the notebook in the current session, you can come back without needing to process the data a second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(\"../data/cache\", \"toxic_comments\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
